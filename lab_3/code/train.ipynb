{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76031434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lib.Transformer import Transformer\n",
    "from lib.TextDataset import TextDataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.lib import load_dataset, collate_fn, generate_story_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6930af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ffc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0.1\n",
    "EPOCHS = 20\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ae138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 60000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "n_encoder_layers=4\n",
    "n_decoder_layers=6\n",
    "max_len = 500\n",
    "pad_idx = 0\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c752a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../models/bpe_tokenizer/tokenizer.json\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    additional_special_tokens=[\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2611908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"../data/train.txt\", tokenizer, data_fraction=N)\n",
    "test_data  = load_dataset(\"../data/test.txt\", tokenizer, data_fraction=N)\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "train_loader = DataLoader(TextDataset(train_data), batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, pad_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c1d0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, d_ff=d_ff, n_encoder_layers=n_encoder_layers, n_decoder_layers=n_decoder_layers, max_len=max_len, pad_idx=pad_idx, dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca07ace",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80245934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    transformer.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        logits = transformer(src, tgt[:, :-1])\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss = {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8edde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../models/transformer/checkpoints/transformer_tinystories.pt\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": transformer.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"epoch\": epoch + 1,\n",
    "    \"loss\": total_loss / len(train_loader),\n",
    "    \"config\": {\n",
    "        \"vocab_size\": transformer.output_linear.out_features,\n",
    "        \"d_model\": transformer.d_model,\n",
    "        \"pad_idx\": transformer.pad_idx,\n",
    "    }\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ed87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../models/transformer/checkpoints/transformer_tinystories.pt\", map_location=device)\n",
    "\n",
    "transformer = Transformer(\n",
    "    vocab_size=checkpoint[\"config\"][\"vocab_size\"],\n",
    "    d_model=checkpoint[\"config\"][\"d_model\"],\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    max_len=max_len,\n",
    "    pad_idx=checkpoint[\"config\"][\"pad_idx\"],\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "transformer.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_story_end(transformer, tokenizer, \"Once upon a time, a cat met a dog\", device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
